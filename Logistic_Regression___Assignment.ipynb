{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YTTOm48894e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "Question 5: Write a Python program that loads a CSV file into a Pandas\n",
        "DataFrame,splits into train/test sets, trains a Logistic Regression model, and\n",
        "prints its accuracy.\n",
        "\n",
        "Program =>\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# 1. Load dataset (from sklearn)\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target   # add target column\n",
        "\n",
        "# 2. Split into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 3. Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)  # increase iterations for convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predictions and Accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy of Logistic Regression model:\", accuracy)\n"
      ],
      "metadata": {
        "id": "FF9uMShxi0ev",
        "outputId": "700ebaad-262c-4af2-ba40-93cd5b3cc231",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression model: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Question 6: Write a Python program to train a Logistic Regression model using L2\n",
        "regularization (Ridge) and print the model coefficients and accuracy.\n",
        "\n",
        "Program =>\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import qload_breast_cancer\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# 2. Features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 3. Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Train Logistic Regression with L2 regularization\n",
        "model = LogisticRegression(penalty=\"l2\", solver=\"liblinear\", max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predictions and accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 6. Model coefficients\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "print(\"Accuracy of Logistic Regression model with L2 regularization:\", accuracy)\n",
        "print(\"\\nFirst 10 coefficients of the model:\\n\", coefficients[:10])"
      ],
      "metadata": {
        "id": "y8j1HmIgj7xT",
        "outputId": "cea6cbbb-cf00-489f-c991-6bd885a44c51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression model with L2 regularization: 0.956140350877193\n",
            "\n",
            "First 10 coefficients of the model:\n",
            " [ 2.13248406e+00  1.52771940e-01 -1.45091255e-01 -8.28669349e-04\n",
            " -1.42636015e-01 -4.15568847e-01 -6.51940282e-01 -3.44456106e-01\n",
            " -2.07613380e-01 -2.97739324e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Question 7: Write a Python program to train a Logistic Regression model for\n",
        "multiclass classification using multi_class='ovr' and print the classification\n",
        "report.\n",
        "\n",
        "Proram =>\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# 1. Load dataset (Iris - multiclass)\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# 2. Features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 3. Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Train Logistic Regression with One-vs-Rest (OvR)\n",
        "model = LogisticRegression(multi_class=\"ovr\", solver=\"liblinear\", max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 6. Classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "id": "QuxN5PchmMAn",
        "outputId": "fe3d26f9-54c6-46a2-d2d5-a1eae4ae5a53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      1.00      1.00         9\n",
            "   virginica       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Question 8: Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "hyperparameters for Logistic Regression and print the best parameters and\n",
        "validation accuracy.\n",
        "\n",
        "Program =>\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# 2. Features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 3. Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Logistic Regression + GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],       # Regularization strength\n",
        "    'penalty': ['l1', 'l2']             # Regularization type\n",
        "}\n",
        "\n",
        "# Use solver that supports both l1 and l2 (liblinear or saga)\n",
        "log_reg = LogisticRegression(solver=\"liblinear\", max_iter=1000)\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=log_reg,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# 5. Results\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Cross-validation Accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "id": "zf34Kb7QnJMT",
        "outputId": "f7517676-008b-4ac9-90be-c11dfc37253f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 100, 'penalty': 'l1'}\n",
            "Best Cross-validation Accuracy: 0.9670329670329672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Question 9: Write a Python program to standardize the features before training\n",
        "Logistic Regression and compare the model's accuracy with and without scaling.\n",
        "\n",
        "Program =>\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# 2. Features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 3. Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ---- Without Scaling ----\n",
        "model_no_scaling = LogisticRegression(max_iter=1000, solver=\"liblinear\")\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# ---- With Scaling ----\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=1000, solver=\"liblinear\")\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# 4. Results\n",
        "print(\"Accuracy without Scaling:\", accuracy_no_scaling)\n",
        "print(\"Accuracy with Scaling   :\", accuracy_scaled)\n"
      ],
      "metadata": {
        "id": "E3Sy8Xe7pXbo",
        "outputId": "5af8c225-180e-4729-c2a1-4900715d5fa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Scaling: 0.956140350877193\n",
            "Accuracy with Scaling   : 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regession | Assignment"
      ],
      "metadata": {
        "id": "qHHIwncG9Zy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "Answer :- 1. Logistic Regression\n",
        "\n",
        "* Logistic Regression is a supervised machine learning algorithm mainly used for classification problems (binary or multi-class).\n",
        "\n",
        "* Instead of predicting a continuous value, it predicts the probability that an observation belongs to a particular class (e.g., \"yes/no\", \"spam/not spam\", \"disease/no disease\").\n",
        "\n",
        "* It uses the logistic (sigmoid) function to map predictions to probabilities between 0 and 1.\n",
        "\n",
        "Sigmoid Function:\n",
        "\n",
        "P(Y=1∣X)=1+e−(β0​+β1​X1​+⋯+βn​Xn​)1​\n",
        "\t​\n",
        "2. Linear Regression\n",
        "\n",
        "* Linear Regression is used for regression problems where the target/output variable is continuous (e.g., predicting house prices, temperature, salary).\n",
        "\n",
        "* It assumes a linear relationship between input variables (X) and the output (Y).\n",
        "* Formula:\n",
        "\n",
        "Y=β0​+β1X1+β2X2+⋯+βnXn+ϵ\n",
        "\n",
        "Question 2: Explain the role of the Sigmoid function in Logistic Regression.\n",
        "\n",
        "Answer:\n",
        "\n",
        "Role of the Sigmoid Function in Logistic Regression\n",
        "\n",
        "In Logistic Regression, we want to model the probability that a given input belongs to a particular class (for example, class = 1 vs class = 0).\n",
        "\n",
        "1. Linear Combination (Raw Score / Logit):\n",
        "\n",
        "Logistic regression first computes a linear combination of the input features:\n",
        "\n",
        "z=w0​+w1​x1​+w2​x2​+⋯+wn​xn​\n",
        "\n",
        "This 𝑧 value can range from − ∞ to +∞.\n",
        "\n",
        "2. Problem with Linear Output:\n",
        "\n",
        "Probabilities must always lie between 0 and 1, but a raw linear function can produce any real number.\n",
        "\n",
        "3. Solution → Sigmoid Function:\n",
        "\n",
        "To convert the linear output into a probability, we pass 𝑧 through the Sigmoid (Logistic) function:\n",
        "\n",
        "σ(z)= 1 / 1+e−z\n",
        "\n",
        "* When 𝑧 → + ∞, σ(z)→1\n",
        "\n",
        "* When z → − ∞, σ(z)→0\n",
        "\n",
        "* When z=0, σ(z)=0.5\n",
        "\n",
        "4. Interpretation:\n",
        "\n",
        "* The output of the sigmoid function is the predicted probability of belonging to the positive class (class = 1).\n",
        "\n",
        "* For example: If σ(z)=0.8, the model predicts an 80% chance that the sample belongs to class 1.\n",
        "\n",
        "5. Decision Boundary:\n",
        "By default, logistic regression uses 0.5 as the threshold:\n",
        "* If σ(z)≥0.5 → Predict class 1\n",
        "* If σ(z)<0.5 → Predict class 0\n",
        "\n",
        "Question 3: What is Regularization in Logistic Regression and why is it needed?\n",
        "\n",
        "Answer: Regularization in Logistic Regression\n",
        "1. What is Regularization?\n",
        "\n",
        "Regularization is a technique used to prevent overfitting in logistic regression (and other models).\n",
        "It works by adding a penalty term to the cost (loss) function, discouraging the model from relying too heavily on any one feature or having excessively large coefficients.\n",
        "\n",
        "2. Logistic Regression without Regularization\n",
        "* Logistic regression tries to minimize the log-loss (cross-entropy loss):\n",
        "\n",
        "J(w)=−m1​i=1∑m​[y(i)log(y^​(i))+(1−y(i))log(1−y^​(i))]\n",
        "\n",
        "* If there are many features, the model may assign very large weights (coefficients) to fit the training data perfectly → this leads to overfitting.\n",
        "\n",
        "3. Regularization in Logistic Regression\n",
        "\n",
        "To control overfitting, we add a penalty term:\n",
        "J(w)=−m1​i=1∑m​[y(i)log(y^​(i))+(1−y(i))log(1−y^​(i))]+λR(w)\n",
        "\n",
        "where:\n",
        "\n",
        "* λ = regularization strength (controls penalty weight).\n",
        "* R(w) = regularization term (depends on type).\n",
        "\n",
        "4. Types of Regularization\n",
        "\n",
        "* L1 Regularization (Lasso):\n",
        "\n",
        "R(w)=∑∣w2j\n",
        "\n",
        "=> Encourages sparsity → some coefficients become exactly zero.\n",
        "\n",
        "=> Useful for feature selection.\n",
        "\n",
        "* L2 Regularization (Ridge):\n",
        "\n",
        "R(w)=∑w2j\t​\n",
        "\n",
        "=> Shrinks weights towards zero (but not exactly zero).\n",
        "\n",
        "=> Helps reduce model complexity and variance.\n",
        "\n",
        "* Elastic Net (Combination of L1 + L2):\n",
        "\n",
        "=> Balances sparsity (L1) and stability (L2).\n",
        "\n",
        "5. Why is Regularization Needed?\n",
        "\n",
        "* Prevents overfitting → model generalizes better to unseen data.\n",
        "\n",
        "* Controls the size of coefficients (avoids extreme weights).\n",
        "\n",
        "* Improves stability and interpretability of the model.\n",
        "\n",
        "* Helps in high-dimensional datasets (many features).\n",
        "\n",
        "Question 4: What are some common evaluation metrics for classification models, and why are they important?\n",
        "\n",
        "Answer :- Common Evaluation Metrics for Classification Models\n",
        "\n",
        "When we build a classification model (like logistic regression), we need ways to measure its performance. Different metrics highlight different aspects of performance.\n",
        "\n",
        "1. Accuracy\n",
        "\n",
        "Accuracy = Correct Predictions/Total Predictions\n",
        "\n",
        "* Tells us the overall proportion of correctly classified samples.\n",
        "* Good when classes are balanced, but misleading for imbalanced datasets.\n",
        "* Example: If 95% of patients are healthy, a model predicting “healthy” always will get 95% accuracy, but it’s useless.\n",
        "\n",
        "2. Precision\n",
        "\n",
        "Precision= TP/ TP+FP\n",
        "\t​\n",
        "* Of all the samples predicted as positive, how many are actually positive?\n",
        "* High precision = fewer false alarms.\n",
        "* Important in applications like spam detection (don’t want to misclassify real emails as spam).\n",
        "\n",
        "3. Recall (Sensitivity / True Positive Rate)\n",
        "\n",
        "Recall=TP/TP+FN\n",
        "\t​\n",
        "* Of all the actual positive cases, how many did the model correctly identify?\n",
        "* High recall = fewer missed positives.\n",
        "* Important in medical diagnosis (don’t want to miss sick patients).\n",
        "\n",
        "4. F1 Score\n",
        "\n",
        "F1=2⋅Precision.Recall/Precision+Recall\n",
        "\t​\n",
        "* Harmonic mean of precision and recall.\n",
        "\n",
        "* Useful when classes are imbalanced.\n",
        "\n",
        "* Balances false positives and false negatives.\n",
        "\n",
        "5. ROC Curve & AUC (Area Under Curve)\n",
        "\n",
        "* ROC Curve plots True Positive Rate (Recall) vs False Positive Rate at different thresholds.\n",
        "\n",
        "* AUC measures overall ability to distinguish between classes.\n",
        "\n",
        "1. AUC = 1 → perfect classifier\n",
        "\n",
        "2. AUC = 0.5 → random guessing\n",
        "\n",
        "6. Confusion Matrix\n",
        "\n",
        "A table that shows counts of:\n",
        "\n",
        "* True Positives (TP)\n",
        "\n",
        "* True Negatives (TN)\n",
        "\n",
        "* False Positives (FP)\n",
        "\n",
        "* False Negatives (FN)\n",
        "\n",
        "Helps visualize what kinds of errors the model makes.\n",
        "\n",
        "Why Are These Metrics Important?\n",
        "\n",
        "* Different applications care about different errors.\n",
        "\n",
        "* Medical diagnosis → Recall is critical (don’t miss positives).\n",
        "\n",
        "* Fraud detection → Precision is critical (don’t wrongly accuse innocent transactions).\n",
        "\n",
        "* Accuracy alone can be misleading in imbalanced datasets.\n",
        "\n",
        "* Metrics guide model improvement (tuning thresholds, regularization, feature engineering).\n",
        "\n",
        "Question 10: Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach you’d take to build a\n",
        "Logistic Regression model — including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business\n",
        "use case.\n",
        "\n",
        "Answer: 1. Data Handling & Preprocessing\n",
        "\n",
        "* Data Cleaning: Handle missing values, outliers, and inconsistencies (e.g., invalid ages, duplicate records).\n",
        "\n",
        "* Feature Engineering: Create meaningful features such as past purchase frequency, recency (last purchase date), average order value, browsing behavior, etc.\n",
        "\n",
        "* Categorical Encoding: Convert categorical variables (like location, product category preference) into numerical form using one-hot encoding or target encoding.\n",
        "\n",
        "2. Feature Scaling\n",
        "\n",
        "* Since Logistic Regression uses gradient descent optimization, scaling helps convergence.\n",
        "\n",
        "* Apply StandardScaler (Z-score normalization) or MinMax scaling to numerical features so that all features contribute equally.\n",
        "\n",
        "3. Handling Class Imbalance (Critical Step)\n",
        "\n",
        "The dataset has only 5% positives (responders), so a naive model would just predict \"non-response\" and achieve 95% accuracy — but that’s useless. Options:\n",
        "\n",
        "1. Resampling Techniques\n",
        "\n",
        "* Oversampling: Use SMOTE or random oversampling to synthetically increase responders.\n",
        "\n",
        "* Undersampling: Reduce majority class (non-responders). Works if dataset is very large.\n",
        "\n",
        "* Hybrid: Combine SMOTE + undersampling.\n",
        "\n",
        "2. Class Weights\n",
        "\n",
        "* Logistic Regression in scikit-learn allows class_weight=\"balanced\" to penalize misclassification of minority class more.\n",
        "\n",
        "* Practical Choice: In business cases like marketing, I’d first try class weights (less risk of overfitting) and validate against SMOTE.\n",
        "\n",
        "4. Hyperparameter Tuning\n",
        "\n",
        "Key Logistic Regression parameters to tune:\n",
        "\n",
        "* Regularization strength (C) → Smaller values = stronger regularization (helps prevent overfitting).\n",
        "\n",
        "* Penalty type → l1, l2, or elasticnet.\n",
        "\n",
        "* Solver → (liblinear, saga) depending on dataset size and penalty.\n",
        "\n",
        "* Use GridSearchCV or RandomizedSearchCV with stratified cross-validation to maintain class ratio in folds.\n",
        "\n",
        "5. Model Evaluation\n",
        "\n",
        "Because of imbalance, accuracy is misleading. Use:\n",
        "\n",
        "* Precision, Recall, F1-score (with more emphasis on Recall if the business wants to minimize missed responders, or Precision if cost of contacting uninterested customers is high).\n",
        "\n",
        "* ROC-AUC: Measures ability to rank positives above negatives.\n",
        "\n",
        "* PR-AUC (Precision-Recall curve): More informative under heavy imbalance.\n",
        "\n",
        "* Confusion Matrix: To see trade-offs at different thresholds.\n",
        "\n",
        "=> Business alignment:\n",
        "\n",
        "* If the cost of sending a marketing message is low, maximize Recall (catch as many responders as possible).\n",
        "\n",
        "* If marketing cost is high (expensive offers), maximize Precision (avoid wasting resources on uninterested customers).\n",
        "\n",
        "6. Deployment & Monitoring\n",
        "\n",
        "* After selecting the best threshold (not default 0.5, but tuned based on Precision-Recall tradeoff), deploy the model.\n",
        "\n",
        "* Continuously monitor drift (customer behavior may change over time).\n",
        "\n",
        "* Retrain periodically as new campaign data comes in.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hGNImjD19meM"
      }
    }
  ]
}